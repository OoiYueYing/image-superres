{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Super Resolution using Autoencoders.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "gJAxdtNX1Yyi",
        "oGLw-cCn1Y0q",
        "ZXsavBJj1Y1N",
        "qMnwvfUb1Y1O",
        "wh9OSw-D1Y1Q",
        "telVTJOQ1Y1n",
        "ksg71gnK1Y13",
        "1QGkU3q21Y2E",
        "aWdvqEyz1Y2L",
        "jEYu9t021Y2Q",
        "8mBRutcQ1Y2R",
        "Ey0Cnp2A1Y2T",
        "J0r-drht1Y3V",
        "JIMWxWjV1Y3Z",
        "1NSeNnml1Y3Z"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilopezfr/image-superres-with-autoencoders-tf/blob/master/Image_Super_Resolution_using_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "x1zxbsCB88JO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ilopezfr/image-superres-with-autoencoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KKA2mwNm1YyI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Super Resolution with Auto-encoders\n",
        "---\n",
        "\n",
        "In this tutorial you will learn how to train an Auto-encoder to convert low-quality images into high-quality. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D4Vz_C6I1YyL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For example, we can take create a high-res image from Hubble telescope (upper) from an original low-res image. \n",
        "\n",
        "<img src=\"img/OE_51_1_011011_f008.png\" width=\"250\" height=\"250\" />"
      ]
    },
    {
      "metadata": {
        "id": "XwL2mP_F1YyM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "0e4my8oH1YyO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is a *low quality* image"
      ]
    },
    {
      "metadata": {
        "id": "brqatpir1YyP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A *low quality* image is often characterized by a low resolution of the sensor. When a photo is taken by a cheap mobile phone, it often looks pixelated and lacks all sorts of details that make up a rich and detailed image.\n",
        "\n",
        "<img src=\"images/pixelatedpics.jpg\"/>\n",
        "\n",
        "Furthermore, an image can be blurry which means that it again lacks high frequency details that are characteristic of high resolution images. An image can become blurry when the shutter speed is too slow and your photo target is moving or if the camera itself is moving and doesn't have any kind of stabilization.\n",
        "\n",
        "<img src=\"images/blurry.png\"/>\n",
        "\n",
        "Finally, an image, especially one taken from a low quality camera, will have some added noise. This happens when a camera is set to a high ISO which therefore increases noise signal with the light signal, this means that the camera will capture more light to illuminate the scene, but graininess will appear more distincly. A situation where this happens is in low light conditions.\n",
        "\n",
        "<img src=\"images/noise.jpg\"/>\n",
        "\n",
        "### During this DLI, we'll be focusing on the first kind of degradation which is low resolution pixelated images.\n",
        "\n",
        "## How to improve a *low quality* image ?\n",
        "\n",
        "Over the years, many methods have been developed. These range from smooth interpolation methods to deconvolution. While giving acceptable results, these methods often rely on some naive assumptions. Sometimes these assumptions are only adapted on a certain type of environment and won't work in a general case.\n",
        "\n",
        "For example, deconvolution appliyed to telescope imaging knows how the blur is spread exactly (exact Point Spread Function) and has a very good idea of the distribution of the noise. So this eases the problem a lot.\n",
        "\n",
        "But what if, we could come up with a method that tries to have a more general vision of such problems ?\n",
        "\n",
        "That's what **AI-based** methods can provide.\n",
        "\n",
        "Specifically, we'll be introducing Autoencoders that are a solution to this problem based on Artificial Neural Networks."
      ]
    },
    {
      "metadata": {
        "id": "FzJKLQrK1YyQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Previous non-AI based methods"
      ]
    },
    {
      "metadata": {
        "id": "S2_43kF71YyR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Filters"
      ]
    },
    {
      "metadata": {
        "id": "VmN3ZKcz1YyS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Many filters exist that can somewhat enhance an image, but the way they enhance the image is fixed, that is, it's done in the same way for any image.\n",
        "\n",
        "For example, let's take this image :\n",
        "\n",
        "<img src=\"images/low_res_no_filter.png\" width=250px/>\n",
        "\n",
        "Now, let's apply a sharpen filter :\n",
        "\n",
        "<img src=\"images/low_res_sharpen.png\" width=250px/>\n",
        "\n",
        "As we can see, no detail is created, we just enhance edges so that the image doesn't look blurry/low res anymore.\n",
        "\n",
        "Let's try to apply cubic interpolation now and compare :\n",
        "\n",
        "<img src=\"images/low_res_cubic.png\" width=250px/>\n",
        "\n",
        "As we can see, this kind of interpolation just smoothes out the image which is even worse than a sharpen filter."
      ]
    },
    {
      "metadata": {
        "id": "N-Voug4p1YyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Deconvolution"
      ]
    },
    {
      "metadata": {
        "id": "0Qp8p4np1YyV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Deconvolution](https://en.wikipedia.org/wiki/Deconvolution) methods often work better on blurry/noisy images and won't be that good in our case, so we won't be delving into them."
      ]
    },
    {
      "metadata": {
        "id": "WKf7q8vy1YyW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**What if we could do better ?** Let's try and beat that."
      ]
    },
    {
      "metadata": {
        "id": "yJzp2PUk1YyX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Autoencoders\n",
        "## What's that ?"
      ]
    },
    {
      "metadata": {
        "id": "j2ecdpMv1YyY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Autoencoders are a kind of neural network that tries to learn a representation of its input data, but in a space with much smaller dimensionality. This smaller representation is able to learn important features of the input data but in a different space which can be then used to reconstruct the data. If you are familiar with [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis), especially [Kernel PCA](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis), the goal is the same, but the way it is achieved and the quality obtained differ greatly, in favor of autoencoders.\n",
        "\n",
        "<img src=\"images/autoencoder_schema.jpg\"/>\n",
        "\n",
        "The fact that they compress data to a smaller feature space, doesn't mean that autoencoders are good at compression (in general). In fact, they're not that good at compression (even compared to JPEG), unless you specifically want to compress a very specific kind of images, let's say trees, or faces or anything specific.\n",
        "\n",
        "Of course, as autoencoders encode the input data to another space, this means that they are somewhat lossy, as can be seen on the above image (reconstructed input).\n",
        "\n",
        "A nice property of autoencoders, is that they adapt to the input and ouput they try to match. This means that if a network can match an input noisy image to a noise-free image, the same network could be used to match input low resolution images to high resolution images, the only difference would be that the networks needs to be retrained using a matching dataset."
      ]
    },
    {
      "metadata": {
        "id": "JIn2aN9J1YyZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What makes up an Autoencoder"
      ]
    },
    {
      "metadata": {
        "id": "oGoPGaU11Yyb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An autoencoder is principally made of a few things :\n",
        "\n",
        "* **An encoder**\n",
        "* **A loss function**\n",
        "* **A decoder**\n",
        "\n",
        "The **encoder** and **decoder** are usually classical neural networks (usually Convolutional Neural Networks, CNNs, in our case).\n",
        "\n",
        "The **encoder** tries to reduce the dimensionality of the input while the **decoder** tries to recover our image from this new space, which is a lossy process.\n",
        "\n",
        "The **loss function** is a way of describing a meaningful difference (or distance) between the input and output. This means that, when training, we'll want to have the smallest possible difference between the input and output, so that the network learns to reconstruct the best possible output."
      ]
    },
    {
      "metadata": {
        "id": "_zh7fSJ51Yyc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What kind of data does an Autoencoder manipulate"
      ]
    },
    {
      "metadata": {
        "id": "1FrT4BPk1Yyh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An autoencoder encodes the data in a smaller feature space, but to find out how to construct our space, the network needs to understand what we want it to do.\n",
        "\n",
        "In fact an autoencoder is a kind of lossy compression which we take advantage of. And here \"*lossy*\" doesn't mean \"*loss of details*\", in fact it just means that we loose information about the original image. We take advantage of that to tell the network to reconstruct the image with more details, which means that we \"lost\" the low quality image information and reconstructed a new sharper image !\n",
        "\n",
        "How can an autoencoder learn to do that ? It needs to find a pattern between a pair of different images. This means that you'll need to show it many different images to tell it *\"Hey net, that's a low res image and that's what it could look like in high res, you get it ?\"*\n",
        "\n",
        "Of course, you'll need to show it many different pairs of low res/high res  so that the network finds a pattern and tries to add more details to your input images.\n",
        "\n",
        "We'll see how to put that in practice below, but before we need to create our network."
      ]
    },
    {
      "metadata": {
        "id": "gJAxdtNX1Yyi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Convolutional Neural Network\n",
        "\n",
        "If you want more details on what are CNNs and what are their layers, you can check out the Appendix notebook which contains an intuitive explanation."
      ]
    },
    {
      "metadata": {
        "id": "yzhGkq2q1Yyj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The encoder"
      ]
    },
    {
      "metadata": {
        "id": "teSLtFhp1Yyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Let's design an encoder"
      ]
    },
    {
      "metadata": {
        "id": "j3K1DewX1Yyl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Designing a neural network is something that needs trial and error to get right.\n",
        "\n",
        "Here we'll be designing the following encoder :"
      ]
    },
    {
      "metadata": {
        "id": "VstTPiLM1Yyo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/model_encoder.png\"/ width=450px>"
      ]
    },
    {
      "metadata": {
        "id": "PX5Q82P01Yyr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note**: the layers' dimensions are noted (depth, width, height)."
      ]
    },
    {
      "metadata": {
        "id": "TrBKy5sJ1Yys",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This might look complicated, but let's have a look at it.\n",
        "\n",
        "The first layer (input_1) takes the input image, here we see that the input image has (3, 256, 256) dimensions. This means that the image is of size 256x256 and has 3 RGB channels (depth)."
      ]
    },
    {
      "metadata": {
        "id": "c2W3Tw8X1Yyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "n = 256\n",
        "chan = 3\n",
        "input_img = Input(shape=(n, n, chan))\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "VEBB5xeR1Yyw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What's interesting here, is that we want the model to reduce the input into another space that, if we visualize, won't necessarily be meaningful to us but will be an **encoded** representation of the input in a space where it has *meaning* for the neural network. **You should note that the *smaller space is not necessarily smaller than one image*, but it should be smaller than all the images that the network trained on, otherwise it's not a kind of compression anymore because we could store every image losslessly**.\n",
        "\n",
        "So, to do that, we use what we saw earlier.\n",
        "\n",
        "Let's first use a convolution :"
      ]
    },
    {
      "metadata": {
        "id": "RGJDnJMX1Yyy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l1 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(input_img)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "LmYpcZRd1Yyz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So here, we put 64 different 3x3 filters. This helps us to retrieve many different features from the input image at its original size, before going down in a smaller space."
      ]
    },
    {
      "metadata": {
        "id": "fn9jr3k71Yy0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's put another convolution layer to strengthen the features we already saw."
      ]
    },
    {
      "metadata": {
        "id": "g7ZnTETp1Yy1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l2 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l1)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "s8BPhbPx1Yy4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, an important step, we begin to descend in a smaller space. We start to slowly downscale the picture by a scale factor of 2."
      ]
    },
    {
      "metadata": {
        "id": "wl3gHJr01Yy6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l3 = MaxPooling2D(padding='same')(l2)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "LuJL3oUH1Yy9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's put a dropout layer here, to be sure that our neurons are already starting to generalize better. So we dropout $30\\%$ of them."
      ]
    },
    {
      "metadata": {
        "id": "F92mr78k1Yy_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l3 = Dropout(0.3)(l3)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "KJhqxMWR1YzC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's do the same as above and use some convolution layers to learn new features in our new and smaller space. We'll try to put even more convolutions (128 convolutions of 3x3). This should be able to learn more features.\n",
        "\n",
        "Why would we want that you say, even though we're in a smaller space ? :O\n",
        "\n",
        "Well, the space is smaller, so we have less information, then let's try to have even more different convolutions that will try and compensate for the loss of information because we went in a smaller space where we lost some information. This gives us much more *perspective*, especially if you imagine these convolution filters as being a *point of view* looking at the same thing (here, the image). *It's like many people (here, the convolution filters) on a team working together and seeing differently the same problem (the image)* (if the analogy helps)"
      ]
    },
    {
      "metadata": {
        "id": "K8wjyCAu1YzE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l4 = Conv2D(128, (3, 3),  padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l3)\n",
        "l5 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l4)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "IqpdrBVW1YzF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, let's descend and get closer to the space where we think we'll find our encoded representation.\n",
        "\n",
        "*Let's dive deeper* :"
      ]
    },
    {
      "metadata": {
        "id": "E6fjI2631YzG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l6 = MaxPooling2D(padding='same')(l5)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "j6uwBepQ1YzG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, let's make sure that every neuron is generalizing and not specializing by droping $50\\%$ of them this time."
      ]
    },
    {
      "metadata": {
        "id": "y5xhThTz1YzH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l3 = Dropout(0.5)(l3)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "WXZNyQ1r1YzI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's dive one last time. This should be enough, we don't want to go too deep because this could become inefficient, both in terms of performance (remember that we need to learn all these convolutions filters) and if you dive so deep that what's only left is a very small representation, for example of size 1x1 (1 pixel), you can't really have too many different *point of views* (convolution filters) giving a different perspective on a single point in space because it is what it is, a single and alone point in space..."
      ]
    },
    {
      "metadata": {
        "id": "zJ5dlrLV1YzJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "encoded = Conv2D(256, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l6)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "5sOUOk341YzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we have our compressed representation that we think will be good enough.\n",
        "\n",
        "Let's just create the encoder's model to be able to train it after. This will enable us to see the **encoded** representation later."
      ]
    },
    {
      "metadata": {
        "id": "xNH8IAm21YzM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "encoder = Model(input_img, encoded)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "Yukv2fVA1YzN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the complete code below, it's the entire encoder. We can see a summary of what our encoder looks like right after."
      ]
    },
    {
      "metadata": {
        "id": "bRaxJR9U1YzP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "49a5182d-622f-4bba-8f82-1fb6eac05b4b"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "\n",
        "n = 256\n",
        "chan = 3\n",
        "input_img = Input(shape=(n, n, chan))\n",
        "\n",
        "l1 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(input_img)\n",
        "l2 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l1)\n",
        "l3 = MaxPooling2D(padding='same')(l2)\n",
        "l3 = Dropout(0.3)(l3)\n",
        "l4 = Conv2D(128, (3, 3),  padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l3)\n",
        "l5 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l4)\n",
        "l6 = MaxPooling2D(padding='same')(l5)\n",
        "l3 = Dropout(0.5)(l3)\n",
        "encoded = Conv2D(256, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l6)\n",
        "encoder = Model(input_img, encoded)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8f7kOjIW1YzW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "4403a023-f6fd-415b-f230-77789478df8e"
      },
      "cell_type": "code",
      "source": [
        "encoder.summary()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 256, 256, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 256, 256, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 256, 256, 64)      36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128, 128, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 128, 128, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 128, 128, 128)     147584    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 64, 64, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 64, 64, 256)       295168    \n",
            "=================================================================\n",
            "Total params: 555,328\n",
            "Trainable params: 555,328\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ng6zThhn1Yzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see, the network looks like the one we described in the above image. Also, there are a lot of parameters (Trainable params) that will need to be trained. Thankfully, the learning algorithm will do the job for us :)"
      ]
    },
    {
      "metadata": {
        "id": "WwgkECqF1Yzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The decoder"
      ]
    },
    {
      "metadata": {
        "id": "St60F-5I1Yzf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now create our decoder !\n",
        "\n",
        "So we want to try doing the steps in **almost reverse order** from our **encoder**."
      ]
    },
    {
      "metadata": {
        "id": "DQLpclKD1Yzh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/model_decoder.png\"/ width=450px>"
      ]
    },
    {
      "metadata": {
        "id": "QQ_1E2nV1Yzj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We just want to come back from the network's representation space to the real world, one we can hopefully understand.\n",
        "\n",
        "So, here we take our **encoded** representation in input and just scale it 2x."
      ]
    },
    {
      "metadata": {
        "id": "j7f30zBG1Yzk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l8 = UpSampling2D()(encoded)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "qXdQhttc1Yzo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're reversing our network here, so let's put some convolutions filters with the same size as the ones before.\n",
        "\n",
        "We'll do it in a \"symmetric\" fashion compared to the encoder.\n",
        "\n",
        "So here, we want to learn to activate neurons on features which learned what is the difference from a pixelated image to a sharper more detailed image in our current space."
      ]
    },
    {
      "metadata": {
        "id": "Pqk1ngHY1Yzq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l9 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l8)\n",
        "l10 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l9)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "JaeojFbI1Yzs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here we'll use a merge layer which performs an add operation on the layer just above it and a layer that was way before.\n",
        "This means that we perform the following operation : convolution2D_4 + convolution2D_7\n",
        "\n",
        "**Note** : convolution2D_4 and convolution2D_7 are the names on the above image.\n",
        "\n",
        "**Why** would we want to do that ?\n",
        "\n",
        "Well, there are a few reasons :\n",
        "\n",
        "* First, imagine that you want to \"share\" knowledge from the encoder to the decoder like \"*Hey, I'm trying to reconstruct this part of the image, did it roughly look like that to you also ?*\".\n",
        "\n",
        "* It also enables the network to not \"loose\" information from going deeper. In fact, when a network becomes too \"deep\", neurons that are at the beginning of the network can start to have difficulty learning because they're too far from the deeper neurons and can't have their knowledge shared. *This called the vanishing gradient problem*, if you want more info, look it up.\n",
        "\n",
        "Anyway, it's just an addition in the end ;)"
      ]
    },
    {
      "metadata": {
        "id": "hdbuCenT1Yzu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l11 = add([l5, l10])\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "EnUK466S1Yzx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can guess what's next, we do the usual steps in reverse :\n",
        "\n",
        "* Come back from the abstract world of the autoencoder (just scale 2x with an Upsampling)\n",
        "* Tell to the neurons that they need to generalize and not specialize (We dropout $30\\%$ of the neurons)\n",
        "* And again try to find interesting features in our recovered and bigger space (with the convolution filters)"
      ]
    },
    {
      "metadata": {
        "id": "Q4efx3PP1Yzz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l12 = UpSampling2D()(l11)\n",
        "l3 = Dropout(0.3)(l3)\n",
        "l13 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l12)\n",
        "l14 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l13)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "OYi358yE1Yz2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, we add the current output from the above layer and a layer that was way before. \n",
        "\n",
        "So we just perform : convolution2D_2 + convolution2D_9"
      ]
    },
    {
      "metadata": {
        "id": "G0FeI3b81Yz3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "l15 = add([l14, l2])\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "aoxhSN2a1Yz5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Finally, we want to ouput our final image with the correct number of channels, which is 3 for RGB."
      ]
    },
    {
      "metadata": {
        "id": "XxUvmxDA1Yz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "# chan = 3, for RGB\n",
        "decoded = Conv2D(chan, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l15)\n",
        "\n",
        "# Create our network\n",
        "autoencoder = Model(input_img, decoded)\n",
        "# You'll understand later what this is\n",
        "autoencoder_hfenn = Model(input_img, decoded)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "gsDS1jEI4fA0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "b8f99230-6a31-4074-cf41-cb0e3ee2e588"
      },
      "cell_type": "code",
      "source": [
        "!wget http://ec2-18-223-16-204.us-east-2.compute.amazonaws.com/lQJp3VdP/tree/data/rez/cars_train\n",
        "  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-25 03:25:52--  http://ec2-18-223-16-204.us-east-2.compute.amazonaws.com/lQJp3VdP/tree/data/rez/cars_train\n",
            "Resolving ec2-18-223-16-204.us-east-2.compute.amazonaws.com (ec2-18-223-16-204.us-east-2.compute.amazonaws.com)... 18.223.16.204\n",
            "Connecting to ec2-18-223-16-204.us-east-2.compute.amazonaws.com (ec2-18-223-16-204.us-east-2.compute.amazonaws.com)|18.223.16.204|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15868 (15K) [text/html]\n",
            "Saving to: ‘cars_train’\n",
            "\n",
            "\rcars_train            0%[                    ]       0  --.-KB/s               \rcars_train          100%[===================>]  15.50K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2019-03-25 03:25:52 (226 KB/s) - ‘cars_train’ saved [15868/15868]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Qzv1ncEo1Yz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Complete network"
      ]
    },
    {
      "metadata": {
        "id": "caGwNoKD1Yz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, let's check out our model.\n",
        "\n",
        "As you can see, it seemed pretty complex, but now that we broke it down, everything seems to make sense. (*If not, I did a bad job :/* )"
      ]
    },
    {
      "metadata": {
        "id": "nE9UQBU-1Yz_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/model.png\"/ width=450px>"
      ]
    },
    {
      "metadata": {
        "id": "mafkd04k1Y0A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here's the final network :"
      ]
    },
    {
      "metadata": {
        "id": "DSI7Mldr1Y0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, Dropout, Conv2DTranspose, UpSampling2D, add\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "\n",
        "# Encoder\n",
        "\n",
        "n = 256\n",
        "chan = 3\n",
        "input_img = Input(shape=(n, n, chan))\n",
        "l1 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(input_img)\n",
        "l2 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l1)\n",
        "\n",
        "l3 = MaxPooling2D(padding='same')(l2)\n",
        "l3 = Dropout(0.3)(l3)\n",
        "l4 = Conv2D(128, (3, 3),  padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l3)\n",
        "l5 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l4)\n",
        "\n",
        "l6 = MaxPooling2D(padding='same')(l5)\n",
        "l3 = Dropout(0.5)(l3)\n",
        "l7 = Conv2D(256, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-HOBKajI1Y0F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Decoder\n",
        "\n",
        "l8 = UpSampling2D()(l7)\n",
        "\n",
        "l9 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l8)\n",
        "l10 = Conv2D(128, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l9)\n",
        "\n",
        "l11 = add([l5, l10])\n",
        "l12 = UpSampling2D()(l11)\n",
        "l3 = Dropout(0.3)(l3)\n",
        "l13 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l12)\n",
        "l14 = Conv2D(64, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l13)\n",
        "\n",
        "l15 = add([l14, l2])\n",
        "\n",
        "# chan = 3, for RGB\n",
        "decoded = Conv2D(chan, (3, 3), padding='same', activation='relu', activity_regularizer=regularizers.l1(10e-10))(l15)\n",
        "\n",
        "# Create our network\n",
        "autoencoder = Model(input_img, decoded)\n",
        "# You'll understand later what this is\n",
        "autoencoder_hfenn = Model(input_img, decoded)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1rpXscY1Y0J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        },
        "outputId": "71d9685e-03ac-42cf-ce4a-756d767aafea"
      },
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 256, 256, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 256, 256, 64) 1792        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 256, 256, 64) 36928       conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 128, 128, 64) 0           max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 128, 128, 128 73856       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 128, 128, 128 147584      conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 64, 64, 256)  295168      max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 128, 128, 128 295040      up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 128, 128, 128 147584      conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 128, 128, 128 0           conv2d_9[0][0]                   \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 128 0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 256, 256, 64) 73792       up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 256, 256, 64) 0           conv2d_14[0][0]                  \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 256, 256, 3)  1731        add_2[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 1,110,403\n",
            "Trainable params: 1,110,403\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGJbiJ8v1Y0O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The final network has even more parameter to train, so this should take some time to train.\n",
        "\n",
        "As you will see, training time really depends on the number of parameters that the network has to train. Of course, it also depends on the size of the input data given. That's why the network you design in the future need to have a balance between performance, number of parameters and a reasonable dataset size.\n",
        "\n",
        "Let's *compile* the model to be able to train it. We'll simply use a Mean Squared Error (MSE) for the loss, we'll see later what this is and if we can go further."
      ]
    },
    {
      "metadata": {
        "id": "SQYqnLHX1Y0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hwHfWGBL1Y0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What happens then, is that we have a dataset (a subset of Image Net for speed reasons) of images and that we load it by batches. Why ?\n",
        "\n",
        "Well, the dataset doesn't really hold in memory, so we split it by batches and give it to the GPU so that it can train on a reasonable part of the dataset at each iteration."
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "fkzqHc5q1Y0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from scipy import ndimage, misc\n",
        "from skimage.transform import resize, rescale\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "\n",
        "def train_batches(just_load_dataset=False):\n",
        "\n",
        "    batches = 256 # Number of images to have at the same time in a batch\n",
        "\n",
        "    batch = 0 # Number if images in the current batch (grows over time and then resets for each batch)\n",
        "    batch_nb = 0 # Batch current index\n",
        "\n",
        "    max_batches = -1 # If you want to train only on a limited number of images to finish the training even faster.\n",
        "    \n",
        "    ep = 4 # Number of epochs\n",
        "\n",
        "    images = []\n",
        "    x_train_n = []\n",
        "    x_train_down = []\n",
        "    \n",
        "    x_train_n2 = [] # Resulting high res dataset\n",
        "    x_train_down2 = [] # Resulting low res dataset\n",
        "    \n",
        "    for root, dirnames, filenames in os.walk(\"/dli/data/rez/cars_train\"):\n",
        "        for filename in filenames:\n",
        "            if re.search(\"\\.(jpg|jpeg|JPEG|png|bmp|tiff)$\", filename):\n",
        "                if batch_nb == max_batches: # If we limit the number of batches, just return earlier\n",
        "                    return x_train_n2, x_train_down2\n",
        "                filepath = os.path.join(root, filename)\n",
        "                image = pyplot.imread(filepath)\n",
        "                if len(image.shape) > 2:\n",
        "                        \n",
        "                    image_resized = resize(image, (256, 256)) # Resize the image so that every image is the same size\n",
        "                    x_train_n.append(image_resized) # Add this image to the high res dataset\n",
        "                    x_train_down.append(rescale(rescale(image_resized, 0.5), 2.0)) # Rescale it 0.5x and 2x so that it is a low res image but still has 256x256 resolution\n",
        "                    batch += 1\n",
        "                    if batch == batches:\n",
        "                        batch_nb += 1\n",
        "\n",
        "                        x_train_n2 = np.array(x_train_n)\n",
        "                        x_train_down2 = np.array(x_train_down)\n",
        "                        \n",
        "                        if just_load_dataset:\n",
        "                            return x_train_n2, x_train_down2\n",
        "                        \n",
        "                        print('Training batch', batch_nb, '(', batches, ')')\n",
        "\n",
        "                        autoencoder.fit(x_train_down2, x_train_n2,\n",
        "                            epochs=ep,\n",
        "                            batch_size=10,\n",
        "                            shuffle=True,\n",
        "                            validation_split=0.15)\n",
        "                    \n",
        "                        x_train_n = []\n",
        "                        x_train_down = []\n",
        "                    \n",
        "                        batch = 0\n",
        "\n",
        "    return x_train_n2, x_train_down2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1BSxLlC51Y0Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you wanted to train the model, that's how you'd do :"
      ]
    },
    {
      "metadata": {
        "id": "Nnn-4MW11Y0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "x_train_n = []\n",
        "x_train_down = []\n",
        "x_train_n, x_train_down = train_batches()\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "j-PtY4Va1Y0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But, we'll just load pretrained model as this takes a lot of time. But first, we'll load the dataset :"
      ]
    },
    {
      "metadata": {
        "id": "EYvpKEm91Y0g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train_n, x_train_down = train_batches(just_load_dataset=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SYeIlN1b1Y0l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And here, we load the already existing weights :"
      ]
    },
    {
      "metadata": {
        "id": "FQUbtXNi1Y0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder.load_weights(\"/dli/data/rez/sr.img_net.mse.final_model5.no_patch.weights.best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGLw-cCn1Y0q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Display the results"
      ]
    },
    {
      "metadata": {
        "id": "r1bczgc81Y0r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You need to load the encoder's weight so that you can see the **encoded** representation :"
      ]
    },
    {
      "metadata": {
        "id": "w3c_XLQn1Y0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder.load_weights('/dli/data/rez/encoder_weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2eyGJauS1Y0w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, we feed the encoder the input images without decoding them so that we can vizualise later the result (and hope to understand how our network encoded our data...)"
      ]
    },
    {
      "metadata": {
        "id": "oZVwJYcc1Y0x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_imgs = encoder.predict(x_train_down)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "enUwafZb1Y00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see, our encoded representation have weird dimensions. We have a representation for each image (256 images), each representation is in 3 dimensions (64x64x256) :"
      ]
    },
    {
      "metadata": {
        "id": "PZI8vbq11Y01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_imgs.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Go6LJrVX1Y03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After this, we encode in our smaller space and then come back and have a super resolution image :"
      ]
    },
    {
      "metadata": {
        "id": "--HPXY-j1Y04",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We clip the output so that it doesn't produce weird colors\n",
        "sr1 = np.clip(autoencoder.predict(x_train_down), 0.0, 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eZapUSRU1Y09",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now run this code below and try to guess which is which. Here are what you shoud guess :\n",
        "\n",
        "* The low resolution input image\n",
        "* The reconstructed image\n",
        "* The original perfect image\n",
        "* The encoded image\n",
        "* A bicubic interopolated version"
      ]
    },
    {
      "metadata": {
        "id": "UJcQd3TM1Y0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_index = 251"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gsn1b7jV1Y1B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Shuffle the pictures so there's more challenge (and you don't cheat :) )\n",
        "shuffled_array = []\n",
        "shuffled_array.append((x_train_down[image_index], \"none\"))\n",
        "shuffled_array.append((x_train_down[image_index], \"bicubic\"))\n",
        "shuffled_array.append((encoded_imgs[image_index].reshape((64*64, 256)), \"none\"))\n",
        "shuffled_array.append((sr1[image_index], \"none\"))\n",
        "shuffled_array.append((x_train_n[image_index], \"none\"))\n",
        "shuffled_array = np.array(shuffled_array)\n",
        "np.random.shuffle(shuffled_array)\n",
        "\n",
        "# Display the images\n",
        "plt.figure(figsize=(128, 128))\n",
        "i = 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oaPOo47p1Y1E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note** : double click on the images to make them bigger"
      ]
    },
    {
      "metadata": {
        "id": "A07qU8dE1Y1F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Type in your guesses\n",
        "\n",
        "*\n",
        "*\n",
        "*\n",
        "*\n",
        "*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kBguyPJr1Y1H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Did you guess correctly ? We'll see below :)\n",
        "\n",
        "Finally, we can see our network bear fruits as we look at the images below.\n",
        "\n",
        "On leftmost side, the input low res image.\n",
        "\n",
        "On the middle-left, we can see an image upscaled using a bicubic interpolation. The result just looks very smoothed out, no details were really added.\n",
        "\n",
        "On the middle (weird violet image) we can see our smaller space representation. If we want to vizualize our smaller space that encodes our images in 3D, we need to convert it to 2D. We do that by merging 2 of the 3 dimensions, which will give us something we can look at (you'll need to zoom in to see something). Let's hope we understand how our network thinks.<details> <summary>(**Spoiler, click on this text at your own risk ;)**)</summary>*We won't...*</details>\n",
        "\n",
        "On the middle-right side, the super resolution recovered image, sharper as expected.\n",
        "\n",
        "On the rightmost, a perfect version of our input image.\n",
        "\n",
        "**But**, you can see that MSE is not that good **but we'll see how to improve upon that if you continue reading ;)**.\n",
        "\n",
        "**Try playing** with the *image_index* variable to see how the model behaves for different images."
      ]
    },
    {
      "metadata": {
        "id": "LuQGC3t91Y1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(128, 128))\n",
        "i = 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index], interpolation=\"bicubic\")\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(encoded_imgs[image_index].reshape((64*64, 256)))\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr1[image_index])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr1[image_index])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RG--2Abv1Y1M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note** : double click on the images to make them bigger"
      ]
    },
    {
      "metadata": {
        "id": "ZXsavBJj1Y1N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    - Legend :  Left (input), Mid-Left (bicubic), Mid (encoding), Mid-Right (output), Right (ground truth)"
      ]
    },
    {
      "metadata": {
        "id": "hrNVUOTx1Y1O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have our final network, let's try to enhance it, but first, we need to understand how we measure image quality between images."
      ]
    },
    {
      "metadata": {
        "id": "qMnwvfUb1Y1O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises :\n",
        "\n",
        "### 1. Try this out yourself and play with the \"image_index\" variable and see what the network outputs.\n",
        "\n",
        "### 2. Try to play with the different layers' parameters and see what happens (improvements/worsening). You won't be able to load the already trained weights as the network won't be the same anymore (unless you only change layers that don't need training).\n",
        "\n",
        "### BONUS 1. Try your own architecture\n",
        "\n",
        "### BONUS 2. Try to understand the encoded space... Good luck :)"
      ]
    },
    {
      "metadata": {
        "id": "wh9OSw-D1Y1Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## How to measure image quality"
      ]
    },
    {
      "metadata": {
        "id": "k9aro1jL1Y1U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To be able to tell apart two images that have a different visual quality and level of detail, we need to introduces metrics that will help us define our **loss function**.\n",
        "\n",
        "During this sections, we'll show how we can understand what metrics tell us about quality loss/change between two images.\n",
        "\n",
        "For this, we'll use this set of images :"
      ]
    },
    {
      "metadata": {
        "id": "bNLti4Eo1Y1V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_index = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X2SD2si31Y1d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, you can play with the \"image_index\" variable to compare the following metrics on different images"
      ]
    },
    {
      "metadata": {
        "id": "eV6CSuXr1Y1e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(128, 128))\n",
        "i = 1\n",
        "\n",
        "# Here, we load the images and display them\n",
        "high_res = x_train_n[image_index]\n",
        "low_res = x_train_down[image_index]\n",
        "\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(high_res)\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(low_res)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D7U0AP4C1Y1m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   -Legend: Left (High res), Right (Low res)"
      ]
    },
    {
      "metadata": {
        "id": "telVTJOQ1Y1n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MSE"
      ]
    },
    {
      "metadata": {
        "id": "3viqcExU1Y1q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) is a metric that indicates perfect similarity if the value is 0.\n",
        "\n",
        "The value grows beyond 0 if the average difference between pixel intensities increases.\n",
        "\n",
        "MSE has a few issues when used for similarity comparison, meaning that if you take an image and photoshop it to make it brighter and compare it with the original, the two images will be very different according to MSE as a dark image has pixel values closer to 0 and a bright image has pixel closer to 1, this means that the difference is going to be very big as MSE sees the image from a very general point of view.\n",
        "\n",
        "Anyway, for our purpose, we compare lower quality images to their higher resolution counterpart, so the brightness is retained, this means that ** MSE is still a useful metric in our case**."
      ]
    },
    {
      "metadata": {
        "id": "D5401v-L1Y1r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mse(orig, res):\n",
        "    return ((orig - res) ** 2).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wda6KVFW1Y1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mse(high_res, low_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ksg71gnK1Y13",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SSIM"
      ]
    },
    {
      "metadata": {
        "id": "ko2ybLmQ1Y14",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Structural similarity (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity) measures the similarity between two images as would be perceived on a television or a similar media."
      ]
    },
    {
      "metadata": {
        "id": "JBQ81FP71Y15",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The SSIM is a value in the range $[1, -1]$ where $1$ would mean two indentical images, and lower values would show a \"perceptual\" difference.\n",
        "\n",
        "This metric compares small windows in the image rather than the whole image (like MSE), which makes it a bit more interesting."
      ]
    },
    {
      "metadata": {
        "id": "w7goFz-91Y17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import skimage.measure\n",
        "import os\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def ssim(ori, res):\n",
        "    return skimage.measure.compare_ssim(ori.astype(np.float64),\n",
        "        res.astype(np.float64),\n",
        "        gaussian_weights=True, data_range=1., win_size=1,\n",
        "        sigma=1.5, multichannel=False, use_sample_covariance=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-7FsuS9A1Y2B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ssim(high_res, low_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1QGkU3q21Y2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### PSNR"
      ]
    },
    {
      "metadata": {
        "id": "xdEHg-OA1Y2E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Peak signal-to-noise ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio) is a metric defined using Mean Squared Error (seen above). \n",
        "\n",
        "*PSNR is most commonly used to measure the quality of reconstruction of lossy compression* - Wikipedia\n",
        "\n",
        "Low resolution and pixelization can be considered as a form of *compression* as we loose information.\n",
        "\n",
        "When there's no noise, the PSNR is infinite (because there's a division by the MSE and MSE is $0$ when both images are exactly the same).\n",
        "\n",
        "Of course, this means that we need to maximize the PSNR. The result is in decibel (dB)."
      ]
    },
    {
      "metadata": {
        "id": "LD7yncvH1Y2G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def psnr(ori, res):\n",
        "    return skimage.measure.compare_psnr(ori, res, data_range=1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3l7-xAxZ1Y2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "psnr(high_res, low_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWdvqEyz1Y2L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### HFENN"
      ]
    },
    {
      "metadata": {
        "id": "_MwI6Zf51Y2M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The HFENN (High Frequency Error Norm Normalized) metric gives  a  measure  of how high-frequency details differ between two images.\n",
        "\n",
        "This means that we can guess if an image has more or less high frequency details (which are fine details that you need to zoom in to see and that are not blurry) compared to another image.\n",
        "\n",
        "When the output value is 0 the images are identical. The greater the value, the more of a perceptual difference in both images there is."
      ]
    },
    {
      "metadata": {
        "id": "FksgXrwa1Y2M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.ndimage import filters\n",
        "\n",
        "def l_o_g(img, sigma):\n",
        "    '''\n",
        "    Laplacian of Gaussian filter (channel-wise)\n",
        "    -> img: input image\n",
        "    -> sigma: gaussian_laplace sigma\n",
        "    <- filtered image\n",
        "    '''\n",
        "    while len(img.shape) < 3:\n",
        "        img = img[..., np.newaxis]\n",
        "    out = img.copy()\n",
        "    for chan in range(img.shape[2]):\n",
        "        out[..., chan] = filters.gaussian_laplace(img[..., chan], sigma)\n",
        "    return out\n",
        "\n",
        "def hfenn(orig, res):\n",
        "    '''\n",
        "    High Frequency Error Norm (Normalized) metric for comparison of original and result images\n",
        "    The metric independent to image size (in contrast to regular HFEN)\n",
        "    Inputs are expected to be float in range [0, 1] (with possible overflow)\n",
        "    -> ori: original image\n",
        "    -> res: result image\n",
        "    <- HFENN value\n",
        "    '''\n",
        "    sgima = 1.5  # From DLMRI paper\n",
        "    return np.mean((l_o_g(orig - res, sgima)) ** 2) * 1e4  # magnification"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0BNf9e7Q1Y2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hfenn(high_res, low_res)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEYu9t021Y2Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises:\n",
        "\n",
        "### 1. Try this out yourself and play with the \"image_index\" variable and try to get a sense of what these metrics mean.\n",
        "\n",
        "### BONUS: Test new metrics that weren't shown here."
      ]
    },
    {
      "metadata": {
        "id": "8mBRutcQ1Y2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Combining losses"
      ]
    },
    {
      "metadata": {
        "id": "equm86ax1Y2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For our current model, we were using the MSE loss which is interesting, but can we do better ?\n",
        "\n",
        "We want to do super resolution, so let's try using a loss that literally doesn't just tell us that there was a difference between our low res image and high res image, but that also tells us that there was an improvement, for example, in high frequency details."
      ]
    },
    {
      "metadata": {
        "id": "1v2c1OCu1Y2S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All the losses have interesting and unique properties. SSIM and RMSE measure image similarity in their own way, PSNR measures the signal reconstruction quality and HFENN measures high frequency details difference. That said, each of these losses bring something interesting to consider over an image.\n",
        "\n",
        "Then... why not **combine** some of them and see what happens ? :)"
      ]
    },
    {
      "metadata": {
        "id": "Ey0Cnp2A1Y2T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MSE and HFENN"
      ]
    },
    {
      "metadata": {
        "id": "gSFMxl7b1Y2U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's try to see what heppens if we combine our MSE with HFENN.\n",
        "\n",
        "First, we need to make it so that our HFENN loss is compatible with Tensorflow/Keras, so here's the code :"
      ]
    },
    {
      "metadata": {
        "id": "B4qFCAo61Y2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.ndimage as nd\n",
        "import scipy.ndimage.filters as filters\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def hfenn_loss(ori, res):\n",
        "    '''\n",
        "    HFENN-based loss\n",
        "    ori, res - batched images with 3 channels\n",
        "    See metrics.hfenn\n",
        "    '''\n",
        "    fnorm = 0.325 # norm of l_o_g operator, estimated numerically\n",
        "    sigma = 1.5 # parameter from HFEN metric\n",
        "    truncate = 4 # default parameter from filters.gaussian_laplace\n",
        "    wradius = int(truncate * sigma + 0.5)\n",
        "    eye = np.zeros((2*wradius+1, 2*wradius+1), dtype=np.float32)\n",
        "    eye[wradius, wradius] = 1.\n",
        "    ker_mat = filters.gaussian_laplace(eye, sigma)\n",
        "    with tf.name_scope('hfenn_loss'):\n",
        "        chan = 3\n",
        "        ker = tf.constant(np.tile(ker_mat[:, :, None, None], (1, 1, chan, 1)))\n",
        "        filtered = tf.nn.depthwise_conv2d(ori - res, ker, [1, 1, 1, 1], 'VALID')\n",
        "        loss = tf.reduce_mean(tf.square(filtered))\n",
        "        loss = loss / (fnorm**2)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PfmHOGTu1Y2b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Then, we want to combine MSE and HFENN. How would we do that ? :O\n",
        "\n",
        "Easily by doing a sum, and even better, we can weight the sum like follows :\n",
        "\n",
        "$MSE + weight * HFENN$, where $weight$ is a real number that could be anything that suits us.\n",
        "\n",
        "We could choose $weight = 10$ and see what happens. Of course this can be tweaked and this value was chosen empirically."
      ]
    },
    {
      "metadata": {
        "id": "rg2ccAOI1Y2e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import losses\n",
        "\n",
        "def ae_loss(input_img, decoder):\n",
        "    mse = losses.mean_squared_error(input_img, decoder) # MSE\n",
        "    weight = 10.0 # weight\n",
        "\n",
        "    return mse + weight * hfenn_loss(input_img, decoder) # MSE + weight * HFENN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "moSB0zLW1Y2g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**That's it !**\n",
        "\n",
        "Now, just compile the model with the new loss :"
      ]
    },
    {
      "metadata": {
        "id": "Xp6XzvYw1Y2h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer='adadelta', loss=ae_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cr6c6AE71Y2l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you wanted to train the model, that's how you'd do :"
      ]
    },
    {
      "metadata": {
        "id": "3Q_MrEDV1Y2l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "```Python\n",
        "x_train_n = []\n",
        "x_train_down = []\n",
        "x_train_n, x_train_down = train_batches()\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "121A8cWL1Y2m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "But, we'll just load the pretrained weights :"
      ]
    },
    {
      "metadata": {
        "id": "ApyDoi-O1Y2q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "autoencoder_hfenn.load_weights(\"/dli/data/rez/sr.img_net.mse_hfenn.final_model5_2.no_patch.weights.best.hdf5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_XqH_53V1Y2t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see what the network can do after using our new custom loss :"
      ]
    },
    {
      "metadata": {
        "id": "J9NuD7zV1Y2u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sr_hfenn = np.clip(autoencoder_hfenn.predict(x_train_down), 0.0, 1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rKHBNKzP1Y21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Again, can you try and guess which image is which ?"
      ]
    },
    {
      "metadata": {
        "id": "Bkn0lK-A1Y25",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here are what you shoud guess :\n",
        "\n",
        "* The low resolution input image\n",
        "* The reconstructed image with MSE\n",
        "* The reconstructed image with our custom MSE + HFENN loss\n",
        "* The original perfect image\n",
        "* A bicubic interopolated version"
      ]
    },
    {
      "metadata": {
        "id": "jETO4hJs1Y26",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_index = 99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5hSKatI71Y29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Shuffle the pictures so there's more challenge (and you don't cheat :) )\n",
        "shuffled_array = []\n",
        "shuffled_array.append((x_train_down[image_index], \"none\"))\n",
        "shuffled_array.append((x_train_down[image_index], \"bicubic\"))\n",
        "shuffled_array.append((sr1[image_index], \"none\"))\n",
        "shuffled_array.append((sr_hfenn[image_index], \"none\"))\n",
        "shuffled_array.append((x_train_n[image_index], \"none\"))\n",
        "shuffled_array = np.array(shuffled_array)\n",
        "np.random.shuffle(shuffled_array)\n",
        "\n",
        "# Display the images\n",
        "plt.figure(figsize=(128, 128))\n",
        "i = 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(shuffled_array[i - 1][0], interpolation=shuffled_array[i - 1][1])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OinV7GWd1Y3D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note** : double click on the images to make them bigger"
      ]
    },
    {
      "metadata": {
        "id": "p5hsXJD41Y3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Type in your guesses\n",
        "\n",
        "*\n",
        "*\n",
        "*\n",
        "*\n",
        "*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_P9Ay0rx1Y3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Did you guess correctly ? The answer's below.\n",
        "\n",
        "As we can see below our custom loss makes it so that the image is more detailed. One thing that we can notice though, is that we loose a tiny bit of brightness in the final image."
      ]
    },
    {
      "metadata": {
        "id": "-axeiJYK1Y3P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_index = 99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vvsWW0QB1Y3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(128, 128))\n",
        "i = 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index], interpolation=\"bicubic\")\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr1[image_index])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr_hfenn[image_index])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_n[image_index])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HSI7em7o1Y3V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note** : double click on the images to make them bigger"
      ]
    },
    {
      "metadata": {
        "id": "J0r-drht1Y3V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    - Legend :  Left (input), Mid-Left (bicubic), Mid (MSE), Mid-Right (MSE + HFENN), Right (Ground truth)"
      ]
    },
    {
      "metadata": {
        "id": "8PDB8wuL1Y3W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you look a bit closely and check out the lines and edges, you'll se that they're sharper when using MSE and HFENN compared to MSE alone."
      ]
    },
    {
      "metadata": {
        "id": "yE28_Swy1Y3X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(128, 128))\n",
        "j = 6\n",
        "i = 1\n",
        "idx_1 = 32*j\n",
        "idx_2 = 32*(j+1)\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_down[image_index, idx_1:idx_2, idx_1:idx_2], interpolation=\"bicubic\")\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr1[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(sr_hfenn[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "i += 1\n",
        "ax = plt.subplot(10, 10, i)\n",
        "plt.imshow(x_train_n[image_index, idx_1:idx_2, idx_1:idx_2])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gQ5F8U6s1Y3Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note** : double click on the images to make them bigger"
      ]
    },
    {
      "metadata": {
        "id": "JIMWxWjV1Y3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###         &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    - Legend :  Left (input), Mid-Left (bicubic), Mid (MSE), Mid-Right (MSE + HFENN), Right (Ground truth)"
      ]
    },
    {
      "metadata": {
        "id": "1NSeNnml1Y3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Exercises :\n",
        "\n",
        "### 1. Try to use a different combination of loss functions and make your own loss."
      ]
    },
    {
      "metadata": {
        "id": "YfZWJN3z1Y3a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# What to remember"
      ]
    },
    {
      "metadata": {
        "id": "z_ZXmNc61Y3a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the end, we were able to harness the power of deep learning and autoencoders to quickly enhance images.\n",
        "\n",
        "What you should remember is :\n",
        "\n",
        "* Autoencoders are deep neural networks\n",
        "* Autoencoders encode your data into a smaller space\n",
        "* Autoencoders find patterns between pairs of images and that is what the encoding represents\n",
        "* The architecture a network is made very empirically\n",
        "* Your loss and the metrics used are very important and influence the way your network learns"
      ]
    },
    {
      "metadata": {
        "id": "nhvOHJ5y1Y3b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "There is another notebook containing more explanations, a \"Going further section\" and bonus exercises if you want to go further."
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "UovOCszJ1Y3b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
      ]
    }
  ]
}